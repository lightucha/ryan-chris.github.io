<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>서포트 벡터 머신 기초(SVM, Support Vector Machine) | 고군분투 대학원생의 데이터 분석</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="본 포스팅은 Bigdata X Yonsei 교육 자료임을 밝힙니다. Support Vector Machine서포트 벡터 머신은 지도학습(Supervised learning)에서 유용하게 쓰이는 알고리즘이다.실습시간을 통해 직접 알고리즘을 구현해 보고 직관을 기르도록 하자. 12345# 실습에 필요한 패키지 불러오기import numpy as npimport">
<meta name="keywords" content="Python,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="서포트 벡터 머신 기초(SVM, Support Vector Machine)">
<meta property="og:url" content="https://ryan-chris.github.io/2018/07/27/SVM_basic/SVM_기초_학생용/index.html">
<meta property="og:site_name" content="고군분투 대학원생의 데이터 분석">
<meta property="og:description" content="본 포스팅은 Bigdata X Yonsei 교육 자료임을 밝힙니다. Support Vector Machine서포트 벡터 머신은 지도학습(Supervised learning)에서 유용하게 쓰이는 알고리즘이다.실습시간을 통해 직접 알고리즘을 구현해 보고 직관을 기르도록 하자. 12345# 실습에 필요한 패키지 불러오기import numpy as npimport">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43298983-6b5632ac-9193-11e8-95eb-45d7c7c10f87.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299000-81ee0f12-9193-11e8-96bd-8b8de7f09db1.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299037-9f77bcfe-9193-11e8-9d5b-c102c9fa15a4.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299047-adc70634-9193-11e8-8519-93ad54754971.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299055-ba41479e-9193-11e8-9329-820f66d31d2c.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299061-c2a374e8-9193-11e8-877e-41b89d69ebb0.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299081-cecb0038-9193-11e8-9b2b-a6ef0f5ec8a5.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299093-dc0b9744-9193-11e8-9c56-37bdc6102d7b.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299109-e6d0738e-9193-11e8-8a5a-3cc0c46d3ba3.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299125-f0dcecea-9193-11e8-9df3-e8744a64810c.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299133-f8f950a8-9193-11e8-8828-966aad6c1cb1.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299144-01d01400-9194-11e8-9ca6-8eb2c13b99a8.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299147-091589a2-9194-11e8-941b-9c3605cb93dd.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299162-12ced2b4-9194-11e8-97fb-bd9290143bab.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299178-1d71cc76-9194-11e8-8096-f4e31d15bd68.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299188-2552c0c6-9194-11e8-89ca-cb9ad4b275fc.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299203-395074f6-9194-11e8-9681-77a35b31e178.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299207-401ef992-9194-11e8-92b6-5949733e51eb.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299214-4a28bf40-9194-11e8-9ffd-5ac693e417f6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299216-51778970-9194-11e8-9354-c4e13054abc3.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299221-5b2f1514-9194-11e8-93a3-27c4c3989bdc.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299226-63ca9932-9194-11e8-9d04-018b51ce5c58.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299274-883f2c74-9194-11e8-8034-f88018a55d59.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/29976233/43299297-93a50d9a-9194-11e8-8eda-e180c2033666.png">
<meta property="og:updated_time" content="2018-07-27T03:05:44.028Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="서포트 벡터 머신 기초(SVM, Support Vector Machine)">
<meta name="twitter:description" content="본 포스팅은 Bigdata X Yonsei 교육 자료임을 밝힙니다. Support Vector Machine서포트 벡터 머신은 지도학습(Supervised learning)에서 유용하게 쓰이는 알고리즘이다.실습시간을 통해 직접 알고리즘을 구현해 보고 직관을 기르도록 하자. 12345# 실습에 필요한 패키지 불러오기import numpy as npimport">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/29976233/43298983-6b5632ac-9193-11e8-95eb-45d7c7c10f87.png">
    

    
        <link rel="alternate" href="/" title="고군분투 대학원생의 데이터 분석" type="application/atom+xml" />
    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">고군분투 대학원생의 데이터 분석</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/about">About</a>
                
                    <a class="main-nav-link" href="/archives">Archives</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/css/images/ryan.png" />
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                    <td><a class="main-nav-link" href="/archives">Archives</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="profile-fixed">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/css/images/ryan.png" />
            <h2 id="name">Chris Cha</h2>
            <h3 id="title">Data Analyst &amp; ML</h3>
            <span id="location"><i class="fa fa-map-marker"></i>Ulsan, Korea</span>
            <a id="follow" target="_blank" href="https://github.com/ryan-chris/">FOLLOW</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                7
                <span>posts</span>
            </div>
            <div class="article-info-block">
                6
                <span>tags</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/ryan-chris/" target="_blank" title="github" class=tooltip>
                            <i class="fa fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/" target="_blank" title="facebook" class=tooltip>
                            <i class="fa fa-facebook"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/" target="_blank" title="instagram" class=tooltip>
                            <i class="fa fa-instagram"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/" target="_blank" title="rss" class=tooltip>
                            <i class="fa fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main"><article id="post-SVM_basic/SVM_기초_학생용" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
            서포트 벡터 머신 기초(SVM, Support Vector Machine)
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2018/07/27/SVM_basic/SVM_기초_학생용/">
            <time datetime="2018-07-27T02:45:03.000Z" itemprop="datePublished">2018-07-27</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/ML/">ML</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/tags/Python/">Python</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <p><em>본 포스팅은 Bigdata X Yonsei 교육 자료임을 밝힙니다.</em></p>
<h1 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h1><p>서포트 벡터 머신은 지도학습(Supervised learning)에서 유용하게 쓰이는 알고리즘이다.<br>실습시간을 통해 직접 알고리즘을 구현해 보고 직관을 기르도록 하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 실습에 필요한 패키지 불러오기</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># notebook 창에서 그래프를 구현할수 있게 해줌.</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/packages_img.png'</span>, width=<span class="number">800</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43298983-6b5632ac-9193-11e8-95eb-45d7c7c10f87.png" alt="Package"></p>
<p>파이썬에는 다양한 주제에 특화된 패키지들이 존재한다.</p>
<ol>
<li>Numpy는 수치연산에 특화된 모듈을 제공</li>
<li>Scikit-learn은 기계학습에 관련된 모듈을 제공</li>
<li>matplotlib은 그래프를 통한 시각화에 특화된 모듈을 제공</li>
<li>IPython은 interactive data visualization에 특화된 모듈을 제공</li>
</ol>
<h3 id="이중에서도-Support-Vector-machine을-구현하는데-도움을-주는-Scikit-learn을-주목해보자"><a href="#이중에서도-Support-Vector-machine을-구현하는데-도움을-주는-Scikit-learn을-주목해보자" class="headerlink" title="이중에서도 Support Vector machine을 구현하는데 도움을 주는 Scikit-learn을 주목해보자"></a>이중에서도 Support Vector machine을 구현하는데 도움을 주는 Scikit-learn을 주목해보자</h3><p>Scikit-learn은 support vector machine을 구현하기 위해 SVC라는 클래스를 제공한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/sklearn_img.png'</span>, width=<span class="number">800</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299000-81ee0f12-9193-11e8-96bd-8b8de7f09db1.png" alt="Scikit-learn"></p>
<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a></p>
<p>sklearn이라는 패키지를 설치하면 svm이라는 서브디렉토리가 존재하고 그 안의 모듈에서 SVC가 클래스로 구현되어 있다.<br>SVC의 parameter를 조정해가며 데이터에 적합한 분류모델을 생성할 수 있다. 직접 실습을 통해 확인해보자.</p>
<h2 id="Classification-Example"><a href="#Classification-Example" class="headerlink" title="Classification Example"></a>Classification Example</h2><p>예측하고자 하는 종속변수가 범주인 “분류(Classification)” 문제를 해결해 보자.<br>간단한 데이터를 직접 생성하여 살펴보도록 하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn 라이브러리에서 제공하는 samples_generator를 사용하여 2분류의 점을 생성하자</span></span><br><span class="line"><span class="comment"># 샘플의 개수는 50개, 집단은 2개, 시드넘버는 0, 집단의 표준편차는 0.6</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">50</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X와 y를 살펴보자</span></span><br><span class="line">print(X), print(y)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.41281595  1.5303347 ]
 [ 1.81336135  1.6311307 ]
 [ 1.43289271  4.37679234]
 [ 1.87271752  4.18069237]
 [ 2.09517785  1.0791468 ]
 [ 2.73890793  0.15676817]
 [ 3.18515794  0.08900822]
 [ 2.06156753  1.96918596]
 [ 2.03835818  1.15466278]
 [-0.04749204  5.47425256]
 [ 1.71444449  5.02521524]
 [ 0.22459286  4.77028154]
 [ 1.06923853  4.53068484]
 [ 1.53278923  0.55035386]
 [ 1.4949318   3.85848832]
 [ 1.1641107   3.79132988]
 [ 0.74387399  4.12240568]
 [ 2.29667251  0.48677761]
 [ 0.44359863  3.11530945]
 [ 0.91433877  4.55014643]
 [ 1.67467427  0.68001896]
 [ 2.26908736  1.32160756]
 [ 1.5108885   0.9288309 ]
 [ 1.65179125  0.68193176]
 [ 2.49272186  0.97505341]
 [ 2.33812285  3.43116792]
 [ 0.67047877  4.04094275]
 [-0.55552381  4.69595848]
 [ 2.16172321  0.6565951 ]
 [ 2.09680487  3.7174206 ]
 [ 2.18023251  1.48364708]
 [ 0.43899014  4.53592883]
 [ 1.24258802  4.50399192]
 [ 0.00793137  4.17614316]
 [ 1.89593761  5.18540259]
 [ 1.868336    0.93136287]
 [ 2.13141478  1.13885728]
 [ 1.06269622  5.17635143]
 [ 2.33466499 -0.02408255]
 [ 0.669787    3.59540802]
 [ 1.07714851  1.17533301]
 [ 1.54632313  4.212973  ]
 [ 1.56737975 -0.1381059 ]
 [ 1.35617762  1.43815955]
 [ 1.00372519  4.19147702]
 [ 1.29297652  1.47930168]
 [ 2.94821884  2.03519717]
 [ 0.3471383   3.45177657]
 [ 2.76253526  0.78970876]
 [ 0.76752279  4.39759671]]
[1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1
 0 1 0 1 0 1 1 0 1 1 0 1 0]





(None, None)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 생성한 점을 그래프 화면에 구현해보도록 하자.(c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x213b1cc0ba8&gt;
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299037-9f77bcfe-9193-11e8-9d5b-c102c9fa15a4.png" alt="figure1"></p>
<blockquote>
<p>colormap <a href="https://matplotlib.org/users/colormaps.html" target="_blank" rel="noopener">https://matplotlib.org/users/colormaps.html</a></p>
</blockquote>
<p>선형분리 classifier는 두 데이터 세트를 분리하는 직선을 그려서 모델을 생성함.<br>위와 같은 2차원 데이터의 경우 손으로도 쉽게 분리할 수 있음.<br>그러나, 어떤 직선이 두 집단을 분류하는 최적의 직선인지 결정하는것은 쉽지 않음</p>
<h3 id="임의로-두-데이터-세트를-분리하는-직선을-그려보기"><a href="#임의로-두-데이터-세트를-분리하는-직선을-그려보기" class="headerlink" title="임의로 두 데이터 세트를 분리하는 직선을 그려보기"></a>임의로 두 데이터 세트를 분리하는 직선을 그려보기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 그려질 직선의 x값에 들어갈 데이터를 준비하자.</span></span><br><span class="line">xfit = np.linspace(<span class="number">-1</span>,<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xfit 살펴보기</span></span><br><span class="line">print(xfit)</span><br></pre></td></tr></table></figure>
<pre><code>[-1.  0.  1.  2.  3.]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에 구현된 그래프를 기본으로 하여 직선을 그리고자 함.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기울기가 m, 절편이 b인 값을 갖는 직선을 반복문을 사용하여 그려보자.</span></span><br><span class="line"><span class="comment"># m의 값에 1, 0.5, -0.2 를 대입하면서, b의 값에 0.65, 1.6, 2.9를 대입하면서 plot을 그린다. k는 선의 색이 black이라는 뜻</span></span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>,<span class="number">0.65</span>), (<span class="number">0.5</span>,<span class="number">1.6</span>), (<span class="number">-0.2</span>,<span class="number">2.9</span>)]:</span><br><span class="line">    plt.plot(xfit, m*xfit+b, <span class="string">'r--'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299047-adc70634-9193-11e8-8519-93ad54754971.png" alt="figure2"></p>
<p>임의로 두 데이터 세트를 분리하는 직선을 그려보았다.<br>직관에 의존하여 데이터를 분리하는 직선을 그렸을때, 어떤 어려움이 있을까?<br>아래의 그림을 구현하여 확인해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에 구현된 그래프를 기본으로 하여 직선을 그리고자 함.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그려질 직선의 x값에 들어갈 데이터를 준비하자.</span></span><br><span class="line">xfit = np.linspace(<span class="number">-1</span>,<span class="number">3.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기울기가 m, 절편이 b인 값을 갖는 직선을 반복문을 사용하여 그려보자.</span></span><br><span class="line"><span class="comment"># m의 값에 1, 0.5, -0.2 를 대입하면서, b의 값에 0.65, 1.6, 2.9를 대입하면서 plot을 그린다. k는 선의 색이 black이라는 뜻</span></span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>,<span class="number">0.65</span>), (<span class="number">0.5</span>,<span class="number">1.6</span>), (<span class="number">-0.2</span>,<span class="number">2.9</span>)]:</span><br><span class="line">    yfit = m * xfit + b</span><br><span class="line">    plt.plot(xfit, yfit, <span class="string">'r--'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 만약 다음과 같은 새로운 데이터가 들어온다면 어떤 라벨을 부여할 것인가?</span></span><br><span class="line"><span class="comment"># 좌표: 0.6, 2.3, 마커 모양: x, 마커 두께: 2, 마커 사이즈: 10</span></span><br><span class="line">plt.plot(<span class="number">0.6</span>,<span class="number">2.3</span>,<span class="string">'x'</span>, color=<span class="string">'blue'</span>, markeredgewidth=<span class="number">2</span>, markersize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x213b2d40860&gt;]
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299055-ba41479e-9193-11e8-9329-820f66d31d2c.png" alt="figure3"></p>
<p>세가지의 선은 모두 두 집단을 완벽하게 구분하고 있다.<br>하지만 X로 표시되는 새로운 데이터가 주어졌을때를 생각해보자.<br>직선에 따라 분류는 다르게 될 것이고, 이를 통해 직감은 충분하지 않음을 확인할 수 있다.</p>
<h2 id="Maximizing-the-Margin"><a href="#Maximizing-the-Margin" class="headerlink" title="Maximizing the Margin"></a>Maximizing the <em>Margin</em></h2><p>서포트 벡터 머신은 최적의 분류선을 결정하는 기준으로 “Margin”이라는 개념을 적용함.<br>집단을 가르는 선 근처의 가장 가까운 양쪽 점(Support Vector) 사이의 거리가 최대가 되도록 하는것이 핵심!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/svm_image1.png'</span>, width=<span class="number">700</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299061-c2a374e8-9193-11e8-877e-41b89d69ebb0.png" alt="figure4"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에 구현된 그래프를 기본으로 하여 직선을 그리고자 함.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그려질 직선의 x값에 들어갈 데이터를 준비하자.</span></span><br><span class="line">xfit = np.linspace(<span class="number">-1</span>,<span class="number">3.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기울기 m, 절편 b 외에 margin을 보여주기 위해 d를 추가로 지정하여 반복문을 생성하자.</span></span><br><span class="line"><span class="keyword">for</span> m, b, d <span class="keyword">in</span> [(<span class="number">1</span>,<span class="number">0.65</span>,<span class="number">0.33</span>), (<span class="number">0.5</span>,<span class="number">1.6</span>,<span class="number">0.55</span>), (<span class="number">-0.2</span>,<span class="number">2.9</span>,<span class="number">0.2</span>)]:</span><br><span class="line">    yfit = m * xfit + b</span><br><span class="line">    plt.plot(xfit, yfit, <span class="string">'r--'</span>)</span><br><span class="line">    plt.fill_between(xfit, yfit-d, yfit+d, edgecolor=<span class="string">'none'</span>, color=<span class="string">'#AAAAAA'</span>, alpha=<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299081-cecb0038-9193-11e8-9b2b-a6ef0f5ec8a5.png" alt="figure5"></p>
<p>위의 그래프에서 세개의 선 중에서 최적의 선을 고르자면 가운데의 선이 될 것.<br>이것은 “maximum margin”의 하나의 예가 될 수 있음.</p>
<h2 id="Fitting-a-support-vector-machine"><a href="#Fitting-a-support-vector-machine" class="headerlink" title="Fitting a support vector machine"></a>Fitting a support vector machine</h2><p>이제 Scikit-Learn이라는 패키지에서 지원하는 서포트 벡터 머신 분류기를 사용해보자.<br>기본적으로, 선형 커널을 사용하고 매개변수 C를 매우 큰 수로 설정하도록 하자.<br>(이것들의 의미는 아래 파트에서 살펴볼 것임)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn 라이브러리의 서브 디렉토리 svm에서 SVC라는 클래스만 불러오도록 하자.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVC의 옵션을 kernel='linear', C=1E10로 지정한 객체 "model"을 생성한다.</span></span><br><span class="line">model = SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">1E10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에서 생성한 X, y 자료를 사용하여 서포트벡터 머신 모델에 피팅시킨다.(학습이 완료됨)</span></span><br><span class="line">model.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;linear&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><p>학습을 통해 어떤 일이 진행되었는지 잘 시각화하기 위해 SVM결정 경계를 그릴수 있는 함수를 이용하자</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVM결정 경계를 그릴수 있는 함수를 사용하자.(다소 긴 관계로 세팅후 사용)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model, ax=None, plot_support=True)</span>:</span></span><br><span class="line">    <span class="string">"""Plot the decision function for a 2D SVC"""</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create grid to evaluate model</span></span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>], xlim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>], ylim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">    Y, X = np.meshgrid(y, x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot decision boundary and margins</span></span><br><span class="line">    ax.contour(X, Y, P, colors=<span class="string">'k'</span>,</span><br><span class="line">               levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], alpha=<span class="number">0.5</span>,</span><br><span class="line">               linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot support vectors</span></span><br><span class="line">    <span class="keyword">if</span> plot_support:</span><br><span class="line">        ax.scatter(model.support_vectors_[:, <span class="number">0</span>],</span><br><span class="line">                   model.support_vectors_[:, <span class="number">1</span>],</span><br><span class="line">                   s=<span class="number">300</span>, linewidth=<span class="number">1</span>, edgecolors=<span class="string">'red'</span>,facecolor=<span class="string">'none'</span>);</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에서 구현했던 그래프를 다시 생성함.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn의 SVC를 사용하여 피팅한 모델의 결정경계를 그려보자.</span></span><br><span class="line">plot_svc_decision_function(model)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299093-dc0b9744-9193-11e8-9c56-37bdc6102d7b.png" alt="figure6"></p>
<p>위는 sklearn의 SVC를 사용하여 피팅한 모델의 결정경계를 나타낸 그래프이다.<br>이 결정경계는 두 데이터 세트 점 사이의 “Margin”을 최대화하는 분리선이다.<br>몇몇 데이터는 점선에 맞닿아 있는데, 이 점들은 ‘Support Vector”라고 부른다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습된 모델에 구해진 서포트 벡터들을 직접 살펴보자</span></span><br><span class="line">model.support_vectors_</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.44359863,  3.11530945],
       [ 2.33812285,  3.43116792],
       [ 2.06156753,  1.96918596]])
</code></pre><p>분류기의 특성은 피팅을 위해 Support vector인 점들만을 이용한다는 점이다.<br>“Margin”의 바깥쪽에 있는 점들은 모델의 피팅에 영향을 주지 않는다.<br>(이론적 내용으로 plus plane과 minus plane의 바깥에 있는 점들은 승수 alpha_i의 값이 0이 되기 때문)  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/Lagrange_img.png'</span>,width=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299109-e6d0738e-9193-11e8-8a5a-3cc0c46d3ba3.png" alt="figure7"></p>
<p>다음과 같은 예를 통해 확인해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make_blobs 함수를 사용하여 각각 샘플이 60개, 120개이며, 집단이 두개로 나뉘는 점을 생성하자.</span></span><br><span class="line">X1, y1 = make_blobs(n_samples=<span class="number">60</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">.6</span>)</span><br><span class="line">X2, y2 = make_blobs(n_samples=<span class="number">120</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">.6</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVC의 옵션을 kernel='linear', C=1E10로 지정한 객체 "model1"과 "model2"를 각각 생성한다.</span></span><br><span class="line">model1 = SVC(kernel=<span class="string">'linear'</span>, C = <span class="number">1E10</span>)</span><br><span class="line">model2 = SVC(kernel=<span class="string">'linear'</span>, C = <span class="number">1E10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에서 생성한 (X1, y1), (X2, y2) 자료를 사용하여 각각 서포트벡터 머신 모델에 피팅시킨다.(학습이 완료됨)</span></span><br><span class="line">model1.fit(X1, y1)</span><br><span class="line">model2.fit(X2, y2)</span><br></pre></td></tr></table></figure>
<pre><code>SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;linear&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 한 화면에 그래프 창이 두개가 뜨도록 만들어 보자.</span></span><br><span class="line"><span class="comment"># plt의 figure라는 클래스를 사용하여 figsize를 (16,6)으로 갖는 fig라는 객체를 생성해둠.</span></span><br><span class="line"><span class="comment"># fig에서 add_subplot이라는 함수를 사용하여 ax1과 ax2라는 두개의 그래프창을 생성함.</span></span><br><span class="line"><span class="comment"># ax1은 (1,2)의 크기를 갖고 1번 자리에 표시, ax2는 (1,2)의 크기를 갖고 2번 자리에 표시</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299125-f0dcecea-9193-11e8-9df3-e8744a64810c.png" alt="figure8"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 한 화면에 그래프 창이 두개가 뜨도록 만들어 보자.</span></span><br><span class="line"><span class="comment"># plt의 figure라는 클래스를 사용하여 figsize를 (16,6)으로 갖는 fig라는 객체를 생성해둠.</span></span><br><span class="line"><span class="comment"># fig에서 add_subplot이라는 함수를 사용하여 ax1과 ax2라는 두개의 그래프창을 생성함.</span></span><br><span class="line"><span class="comment"># ax1은 (1,2)의 크기를 갖고 1번 자리에 표시, ax2는 (1,2)의 크기를 갖고 2번 자리에 표시</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 두개의 그래프 창에 각각 샘플이 60개일때와 120개일때의 그림을 그려본다.</span></span><br><span class="line"><span class="comment"># ax1에 X1데이터의 첫번째 변수와 두번째 변수를 사용하는 scatter plot을 생성, c=color, s=점의 크기, cmap=colormap</span></span><br><span class="line"><span class="comment"># 그래프의 제목은 'N=60'이라 함.</span></span><br><span class="line">ax1.scatter(X1[:,<span class="number">0</span>], X1[:,<span class="number">1</span>], c=y1, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line">plot_svc_decision_function(model1, ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'N=60'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ax2에 X2데이터의 첫번째 변수와 두번째 변수를 사용하는 scatter plot을 생성, c=color, s=점의 크기, cmap=colormap</span></span><br><span class="line"><span class="comment"># 그래프의 제목은 'N=120'이라 함.</span></span><br><span class="line">ax2.scatter(X2[:,<span class="number">0</span>], X2[:,<span class="number">1</span>], c=y2, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line">plot_svc_decision_function(model2, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">'N=120'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.text.Text at 0x213b3354518&gt;
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299133-f8f950a8-9193-11e8-8828-966aad6c1cb1.png" alt="figure9"></p>
<p>왼쪽 그래프는 60개 학습데이터에 대한 모델의 결정경계선을 보여주고,<br>오른쪽 그래프는 120개 학습데이터에 대한 결정경계선을 보여준다.<br>학습데이터가 더 추가되었음에도, 결정경계선에 가장 가까운 데이터가 변하지 않는다면 모델은 동일하게 유지된다.</p>
<h2 id="Beyond-linear-boundaries-Kernel-SVM"><a href="#Beyond-linear-boundaries-Kernel-SVM" class="headerlink" title="Beyond linear boundaries: Kernel SVM"></a>Beyond linear boundaries: Kernel SVM</h2><p>어떤 데이터는 주어진 차원공간 안에서 단순하게 분리가 어려운 경우가 있다.<br>다음으로 소개할 커널 SVM은 그러한 경우에 사용될 수 있는 효과적인 모델이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn 라이브러리의 samples_generator에서 두 부류의 원모양 데이터를 제공하는 make_circles 함수를 불러오자.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_circles</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 샘플개수 500개, factor(바깥쪽 원과 안쪽 원 사이의 Scale) =.1로 지정, noise(데이터의 표준편차)=.1로 지정하여 데이터 생성.  </span></span><br><span class="line">X, y = make_circles(n_samples=<span class="number">500</span>, factor=<span class="number">.1</span>, noise=<span class="number">.1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 생성된 데이터를 그래프에 표현.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x213b345a198&gt;
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299144-01d01400-9194-11e8-9ca6-8eb2c13b99a8.png" alt="figure10"></p>
<p>위와 같은 모습을 띄는 데이터를 단순 선형분리 svm을 사용하여 결정경계를 그려보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 생성된 데이터를 그래프에 표현.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'linear' 커널을 옵션으로 지정한 SVC의 인스턴스 model_circle 생성</span></span><br><span class="line">model_circle = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 앞서 생성한 X, y를 model_circle에 피팅</span></span><br><span class="line">model_circle.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 생성한 모델의 결정경계선 그리기</span></span><br><span class="line">plot_svc_decision_function(model_circle,plot_support=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299147-091589a2-9194-11e8-941b-9c3605cb93dd.png" alt="figure11"></p>
<p>위와 같은 모습을 띄는 두 부류의 데이터는 기존의 선형분리선으로는 분류하기 어렵다.<br>만약 이 데이터를 더 높은 차원으로 투영 할 수 있다면 어떨까?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X[:,0]에 해당하는 x1과 X[:,1]에 해당하는 x2를 사용하여 새로운 차원의 r을 생성</span></span><br><span class="line">r=np.exp(-(X**<span class="number">2</span>)).sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>기존 2차원에 존재하던 데이터를 3차원 상에 투영시켜 시각화 해보도록 하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3차원 그래프를 사용하기 위해 mpl_toolkits.mplot3d에서 Axes3D라는 모듈을 불러온다.</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plt의 figure 클래스를 사용해서 크기가 (9,6)인 fig라는 객체를 생성해 놓는다.</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># fig에서 add_subplot이라는 함수를 사용하여 3차원 공간을 쓰는 ax1을 생성해 놓는다.</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xs에는 X[:,0]의 값을 ys에는 X[:,1]의 값을, zs에는 r값을 넣는 3차원 scatter plot을 생성한다.</span></span><br><span class="line"><span class="comment"># c=color, cmap=colormap</span></span><br><span class="line">ax.scatter(xs=X[:,<span class="number">0</span>], ys=X[:,<span class="number">1</span>], zs=r, c=y, cmap=<span class="string">'summer'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x213b36eb9e8&gt;
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299162-12ced2b4-9194-11e8-97fb-bd9290143bab.png" alt="figure12"></p>
<p>추가적인 차원을 사용함에 따라 이 데이터는 ‘r=0.7’부근을 기점으로 분리될수 있다.<br>다만, 항상 위의 예제처럼 손쉽게 변환 함수를 직접 구할수 있는 것이 아니기 때문에, 라이브러리를 사용하여보자.<br>sklearn 라이브러리에서는 적절한 옵션을 지정하여 커널 svm을 구현해 볼 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kernel='rbf'로 지정하고 gamma='auto', C=1E10으로 하는 model_kernel이라는 객체를 생성하자.</span></span><br><span class="line">model_kernel = SVC(kernel=<span class="string">'rbf'</span>, gamma=<span class="string">'auto'</span>, C=<span class="number">1E10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model_kernel의 fit함수를 사용하여 (X,y)데이터에 대한 커널 SVM모델을 생성하자.</span></span><br><span class="line">model_kernel.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;rbf&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><p>기존의 scattter plot을 그리고, model_kernel을 사용한 결정경계가 추가로 그려지도록 해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기존의 scatter plot을 그리자.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel을 결정경계를 그려주는 함수인 plot_svc_decision_function에 넣는다.</span></span><br><span class="line">plot_svc_decision_function(model_kernel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel의 서포트 벡터들을 표시해주도록 한다. 마커의 크기는 300, 두께는 1, 마커 속의 색은 비우기</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299178-1d71cc76-9194-11e8-8096-f4e31d15bd68.png" alt="figure13"></p>
<p>위와 같이 커널 서포트 벡터 머신을 사용하여 적절한 비선형 결정경계를 학습할 수 있으며,<br>SVC()의 parameter인 ‘gamma’를 조절하여, 결정경계를 타이트하게 학습할 것인지, 여유있게 학습할 것인지 정할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/rbf_kernel_img.png'</span>,width=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299188-2552c0c6-9194-11e8-89ca-cb9ad4b275fc.png" alt="가우시안 커널"></p>
<p>감마는 가우시안 RBF의 커널에 사용되는 매개변수인데, 그 값으로 가우시안 커널의 폭을 조절할 수 있다.<br>감마값이 작을수록 모델의 복잡도가 낮고, 감마값이 클수록 모델의 복잡도가 높다는 정도만 알아도록 하자.</p>
<blockquote>
<p>gamma &amp; C referenct: <a href="https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine" target="_blank" rel="noopener">https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine</a></p>
</blockquote>
<blockquote>
<blockquote>
<p>C is the parameter for the soft margin cost function, which controls the influence of each individual support vector; this process involves trading error penalty for stability.</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>If gamma is large, then variance is small implying the support vector does not have wide-spread influence. Technically speaking, large gamma leads to high bias and low variance models, and vice-versa.</p>
</blockquote>
</blockquote>
<h3 id="가우시안-RBF커널의-gamma-값을-작게-지정했을-때-gamma-1"><a href="#가우시안-RBF커널의-gamma-값을-작게-지정했을-때-gamma-1" class="headerlink" title="가우시안 RBF커널의 gamma 값을 작게 지정했을 때 : gamma = 1"></a>가우시안 RBF커널의 gamma 값을 작게 지정했을 때 : gamma = 1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kernel='rbf'로 지정하고 gamma=1, C=1E10으로 하는 model_kernel이라는 객체를 생성하자.</span></span><br><span class="line">model_kernel = SVC(kernel=<span class="string">'rbf'</span>, gamma=<span class="number">1</span>, C=<span class="number">1E10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel의 fit함수를 사용하여 (X,y)데이터에 대한 커널 SVM모델을 생성하자.</span></span><br><span class="line">model_kernel.fit(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기존의 scatter plot을 그리자.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel을 결정경계를 그려주는 함수인 plot_svc_decision_function에 넣는다.</span></span><br><span class="line">plot_svc_decision_function(model_kernel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel의 서포트 벡터들을 표시해주도록 한다. 마커의 크기는 300, 두께는 1, 마커 속의 색은 비우기</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299203-395074f6-9194-11e8-9681-77a35b31e178.png" alt="figure14"></p>
<h3 id="가우시안-RBF커널의-gamma-값을-크게-지정했을-때-gamma-10"><a href="#가우시안-RBF커널의-gamma-값을-크게-지정했을-때-gamma-10" class="headerlink" title="가우시안 RBF커널의 gamma 값을 크게 지정했을 때 : gamma = 10"></a>가우시안 RBF커널의 gamma 값을 크게 지정했을 때 : gamma = 10</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kernel='rbf'로 지정하고 gamma=10, C=1E10으로 하는 model_kernel이라는 객체를 생성하자.</span></span><br><span class="line">model_kernel = SVC(kernel=<span class="string">'rbf'</span>, gamma=<span class="number">10</span>, C=<span class="number">1E10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel의 fit함수를 사용하여 (X,y)데이터에 대한 커널 SVM모델을 생성하자.</span></span><br><span class="line">model_kernel.fit(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기존의 scatter plot을 그리자.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel을 결정경계를 그려주는 함수인 plot_svc_decision_function에 넣는다.</span></span><br><span class="line">plot_svc_decision_function(model_kernel)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model_kernel의 서포트 벡터들을 표시해주도록 한다. 마커의 크기는 300, 두께는 1, 마커 속의 색은 비우기</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299207-401ef992-9194-11e8-92b6-5949733e51eb.png" alt="figure15"></p>
<blockquote>
<p>gamma 값이 커질수록 모형의 복잡도가 커진다(과적합 우려)</p>
</blockquote>
<h2 id="Tuning-the-SVM-Softening-Margins"><a href="#Tuning-the-SVM-Softening-Margins" class="headerlink" title="Tuning the SVM: Softening Margins"></a>Tuning the SVM: Softening Margins</h2><p>앞서 수행한 학습들은 분류하고자 하는 두 집단이 깨끗하게 분리되어 있는 상황에서 진행되었다.<br>그러나 데이터가 약간의 중복이 존재하거나 이상치가 존재한다면 어떨까?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/svm_image3.png'</span>, width=<span class="number">600</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299214-4a28bf40-9194-11e8-9ffd-5ac693e417f6.png" alt="figure16"></p>
<p>위와 같은 경우, 오른쪽과 같이 엄격하게 접근하여 결정경계면을 정해야 할까?<br>아니면 왼쪽과 같이 오분류를 허용하더라도 넓은 Margin을 확보해야 할까?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/svm_image4.png'</span>, width=<span class="number">600</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299216-51778970-9194-11e8-9354-c4e13054abc3.png" alt="figure17"></p>
<p>경우에 따라 다르겠지만, 실제 데이터 분포가 위와같은 경우였다면 오른쪽의 엄격한 접근은 좋지않아 보인다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 결정경계가 모호한 데이터 셋을 생성하여 보자.</span></span><br><span class="line"><span class="comment"># 샘플의 개수는 200개, 집단은 2개, 시드넘버는 0, 집단의 표준편차는 0.8</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">200</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">.8</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 해당 데이터셋을 scatter plot으로 표현해 보자.</span></span><br><span class="line"><span class="comment"># (X의 첫번째 열과 두번째 열을 사용, c는 점의 색깔, s는 점의 크기, cmap은 colormap)</span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기울기가 m, 절편이 b인 값을 갖는 직선을 반복문을 사용하여 그려보자.</span></span><br><span class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">0.7</span>,<span class="number">1.7</span>), (<span class="number">0.2</span>,<span class="number">2.15</span>)]:</span><br><span class="line">    yfit = m * xfit + b</span><br><span class="line">    plt.plot(xfit, yfit, <span class="string">'r--'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299221-5b2f1514-9194-11e8-93a3-27c4c3989bdc.png" alt="figure18"></p>
<p>전통적인 SVM 방식은 하드마진(Hard Margin)방법이라고 한다.<br>하드마진 방식은 매우 엄격하게 두개의 클래스를 분리하는 방법으로, 모든 입력값은 반드시 한 클래스에 속해야 한다.<br>하지만 몇개의 노이즈로 인해 두 그룹을 구별하는 초평면을 구하기가 어려운 경우가 존재한다.  </p>
<p>소프트마진(Soft Margin)방법은 경계선을 두고 약간의 오류를 인정하는 방법이다.<br>C값을 조정가능한 값으로 설정하여 오류를 인정하는 정도를 결정해 줄 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'images/Softmargin_img.png'</span>, width=<span class="number">500</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299226-63ca9932-9194-11e8-9d04-018b51ce5c58.png" alt="objective function"></p>
<p>위는 소프트마진 SVM의 경우 최적화 시켜야 하는 목적함수를 나타내는 것이다.<br>오른쪽 항의 C가 커질수록 슬랙변수에 대한 민감도가 커져서 마진폭이 줄어듬.<br>실습을 통해 C값을 크게 지정한 경우와, 작게 지정한 경우를 비교해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에서 살펴본 데이터셋을 다시 생성한다.</span></span><br><span class="line"><span class="comment"># 샘플의 개수는 200개, 집단은 2개, 시드넘버는 0, 집단의 표준편차는 0.8</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">200</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">.8</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVC의 옵션을 kernel='linear', C=10, C=0.5로 지정한 객체 "model1"과 "model2"를 각각 생성한다.</span></span><br><span class="line">model1 = SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">10</span>)</span><br><span class="line">model2 = SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">.5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위에서 생성한 (X, y) 자료를 사용하여 각각 서포트벡터 머신 모델에 피팅시킨다.(학습이 완료됨)</span></span><br><span class="line">model1.fit(X, y)</span><br><span class="line">model2.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;linear&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 한 화면에 그래프 창이 두개가 뜨도록 만들어 보자.</span></span><br><span class="line"><span class="comment"># plt의 figure라는 클래스를 사용하여 figsize를 (20,6)으로 갖는 fig라는 객체를 생성해둠.</span></span><br><span class="line"><span class="comment"># fig에서 add_subplot이라는 함수를 사용하여 ax1과 ax2라는 두개의 그래프창을 생성함.</span></span><br><span class="line"><span class="comment"># ax1은 (1,2)의 크기를 갖고 1번 자리에 표시, ax2는 (1,2)의 크기를 갖고 2번 자리에 표시</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/29976233/43299274-883f2c74-9194-11e8-8034-f88018a55d59.png" alt="figure19"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 한 화면에 그래프 창이 두개가 뜨도록 만들어 보자.</span></span><br><span class="line"><span class="comment"># plt의 figure라는 클래스를 사용하여 figsize를 (20,6)으로 갖는 fig라는 객체를 생성해둠.</span></span><br><span class="line"><span class="comment"># fig에서 add_subplot이라는 함수를 사용하여 ax1과 ax2라는 두개의 그래프창을 생성함.</span></span><br><span class="line"><span class="comment"># ax1은 (1,2)의 크기를 갖고 1번 자리에 표시, ax2는 (1,2)의 크기를 갖고 2번 자리에 표시</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>,<span class="number">6</span>))</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 두개의 그래프 창에 각각 C=10일때와, 0.5 때의 그림을 그려본다.</span></span><br><span class="line"><span class="comment"># X의 첫번째 변수와 두번째 변수를 사용하는 scatter plot을 그림, c=color, s=점의 크기, cmap=colormap</span></span><br><span class="line"><span class="comment"># C=10일때의 model1을 사용, ax=ax1사용, 결정경계를 그린다.</span></span><br><span class="line"><span class="comment"># 그래프의 제목을 'C = 10'이라 한다.</span></span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line">plot_svc_decision_function(model1, ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'C=10'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># X의 첫번째 변수와 두번째 변수를 사용하는 scatter plot을 그림, c=color, s=점의 크기, cmap=colormap</span></span><br><span class="line"><span class="comment"># C=0.5일때의 model1을 사용, ax=ax1사용, 결정경계를 그린다.</span></span><br><span class="line"><span class="comment"># 그래프의 제목을 'C = 0.5'라 한다.</span></span><br><span class="line">ax2.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'summer'</span>)</span><br><span class="line">plot_svc_decision_function(model2, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">'C=.5'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.text.Text at 0x213b2dfc518&gt;
</code></pre><p><img src="https://user-images.githubusercontent.com/29976233/43299297-93a50d9a-9194-11e8-8eda-e180c2033666.png" alt="figure20"></p>
<p>C값이 클수록 더 엄격하게 분할 띠를 형성하며, C값이 작을때는 오류를 어느정도 허용하면서 분할 띠를 형성한다.</p>
<p>Cost Reference: <a href="https://ratsgo.github.io/machine%20learning/2017/05/29/SVM2/" target="_blank" rel="noopener">https://ratsgo.github.io/machine%20learning/2017/05/29/SVM2/</a></p>
<p>다음은 실제 데이터에 적용해 보는 실습을 해보겠습니다.</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://ryan-chris.github.io/2018/07/27/SVM_basic/SVM_기초_학생용/" data-id="cjk3e0hqq00006om3r7j8moq1" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fa fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fa fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fa fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fa fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2018/07/28/SVM_basic/SVM_실데이터 활용_최종/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    서포트 벡터 머신 활용(IRIS)
                
            </div>
        </a>
    
    
        <a href="/2018/07/17/basic-pandas-3/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">판다스 기초 03</div>
        </a>
    
</nav>


    
</article>


    
    

</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">recent</h3>
        <div class="widget">
            <ul id="recent-post" class="">
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/08/09/git_pr/" class="thumbnail">
    
    
        <span style="background-image:url(https://user-images.githubusercontent.com/29976233/43906946-691e71d8-9c2f-11e8-8b7d-270548463431.png)" alt="초짜의 Git Pull-Request(PR)" class="thumbnail-image"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Git/">Git</a></p>
                            <p class="item-title"><a href="/2018/08/09/git_pr/" class="title">초짜의 Git Pull-Request(PR)</a></p>
                            <p class="item-date"><time datetime="2018-08-09T14:00:03.000Z" itemprop="datePublished">2018-08-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/07/28/SVM_basic/SVM_실데이터 활용_최종/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/ML/">ML</a></p>
                            <p class="item-title"><a href="/2018/07/28/SVM_basic/SVM_실데이터 활용_최종/" class="title">서포트 벡터 머신 활용(IRIS)</a></p>
                            <p class="item-date"><time datetime="2018-07-28T02:00:03.000Z" itemprop="datePublished">2018-07-28</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/07/27/SVM_basic/SVM_기초_학생용/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/ML/">ML</a></p>
                            <p class="item-title"><a href="/2018/07/27/SVM_basic/SVM_기초_학생용/" class="title">서포트 벡터 머신 기초(SVM, Support Vector Machine)</a></p>
                            <p class="item-date"><time datetime="2018-07-27T02:45:03.000Z" itemprop="datePublished">2018-07-27</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/07/17/basic-pandas-3/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a></p>
                            <p class="item-title"><a href="/2018/07/17/basic-pandas-3/" class="title">판다스 기초 03</a></p>
                            <p class="item-date"><time datetime="2018-07-17T14:50:03.000Z" itemprop="datePublished">2018-07-17</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-thumbnail">
                            <a href="/2018/07/16/basic-pandas-2/" class="thumbnail">
    
    
        <span class="thumbnail-image thumbnail-none"></span>
    
</a>

                        </div>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Python/">Python</a></p>
                            <p class="item-title"><a href="/2018/07/16/basic-pandas-2/" class="title">판다스 기초 02</a></p>
                            <p class="item-date"><time datetime="2018-07-16T13:18:03.000Z" itemprop="datePublished">2018-07-16</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">3</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PR/">PR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pull-Request/">Pull-Request</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/">git</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/Machine-Learning/" style="font-size: 13.33px;">Machine Learning</a> <a href="/tags/PR/" style="font-size: 10px;">PR</a> <a href="/tags/Pandas/" style="font-size: 16.67px;">Pandas</a> <a href="/tags/Pull-Request/" style="font-size: 10px;">Pull-Request</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/git/" style="font-size: 10px;">git</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2018 Ukhyeon Chris Cha<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        


    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>